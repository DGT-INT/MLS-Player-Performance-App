---
title: "04 EDA"
author: "Daniel Tshiani"
date: "2026-01-04"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
```

```{r}
working_data <- readRDS("../data/03 working data season 2025.rds")
```

```{r}
sapply(working_data, function(x) {
  if (is.factor(x)) length(unique(x))
})
```

```{r}
# limited season and competition to 1 so i am filtering them out
working_data <- working_data %>%
  select(-season, # limited to only 2025 season
         -competition, # limited to only MLS
         -`player name`, # irrelevant for modeling
         -position, # already have general position
         -`base salary`, # nearly replica of dependent variable
         -`date of birth`, # already have age in the model
         -nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
         ) %>%
  mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
  select(-`height (ft)`,
         -`height (in)`,
         -`games played`, # correlated to minutes played
         -guaranteed_compensation,
         -`goals + assists`, # already have both variables in the model
         -`primary assists - exp. assists`, # already have both variables in the model
         -`team name` # i want the model to learn from the individual players rather than the collected team
         ) %>%
  mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
  select(
    -`gauranteed compensation` 
  )

```

```{r}
set.seed(123456)

train_index <- createDataPartition(
  working_data$log_salary,
  p = 0.8,
  list = FALSE
)

train_data <- working_data[train_index, ]
test_data <- working_data[-train_index, ]

nrow(train_data)
nrow(test_data)
```


# Baseline model

```{r}
lm_baseline <- lm(log_salary ~ .,
                  data = train_data)

summary(lm_baseline)
```

```{r}
preds <- predict(lm_baseline, newdata = test_data)
lm_RMSE <- RMSE(preds, test_data$log_salary)
lm_RMSE
```

RMSE is used to evaluate models because it captures the typical size of prediction errors and discourages large misses.

```{r}
lm_MAE <- MAE(preds, test_data$log_salary)
lm_MAE
```
MAE is reported alongside RMSE to show the average size of prediction errors without giving extra weight to large outliers.

R² is included to provide context on how much of the variation in salary is explained by the model.
```{r}
lm_R2 <- R2(preds, test_data$log_salary)
lm_R2
```

Adjusted R² is used for linear models to account for model complexity and ensure that additional predictors meaningfully improve model fit.
```{r}
lm_ADJR2 <- summary(lm_baseline)$adj.r.squared
lm_ADJR2
```

```{r}
evaluation <- data.frame(
  model = "Linear Regression",
  RMSE = lm_RMSE,
  MAE = lm_MAE,
  R_squared = lm_R2,
  Adjusted_R_squared = lm_ADJR2
)

evaluation
```


# Random Forest

```{r}
# i was having syntatic issues so this resolves it.
names(train_data) <- make.names(names(train_data))
names(test_data)  <- make.names(names(test_data))
```


```{r}
rf_model <- randomForest(
  log_salary ~.,
  data = train_data,
  ntree = 500,
  mtry = floor(sqrt(ncol(train_data)-1)),
  importance = TRUE
)

rf_model
```

```{r}
importance(rf_model)
```

```{r}
varImpPlot(rf_model)
```

Salary is strongly influenced by experience (age), roster designation (homegrown), and on-field involvement (minutes played, share of team touches, attempted passes). At first i was suprised that a metric like assist had less of an influence on salary however this model includes all positons. So for goalkeepers, defenders, and attackers who are goal oriented dont focus on assists. In the future, it probably more appropriate accomadate more for different positions.

```{r}
preds_rf <- predict(rf_model, newdata = test_data)

rf_RMSE <- RMSE(preds_rf, test_data$log_salary)
rf_MAE <- MAE(preds_rf, test_data$log_salary)
rf_R2 <- R2(preds_rf, test_data$log_salary)

evaluation <- rbind(
  evaluation,
  data.frame(
  model = "Random Forest",
  RMSE = rf_RMSE,
  MAE = rf_MAE,
  R_squared = rf_R2,
  Adjusted_R_squared = NA #I'll come back to this if its relevant
  )
)

evaluation
```

Compared to the baseline linear regression, the Random Forest model achieved lower RMSE and MAE while also explaining a greater proportion of variance in salary. This suggests that non-linear relationships and interactions between player characteristics play an important role in determining guaranteed compensation.

Given its improved predictive accuracy across all evaluation metrics, the Random Forest model will selected over Linear Regression as the final model for deployment.

```{r}
# testing different parameters to see if i should dedicate time on tuning the random forest

p <- ncol(train_data) - 1  

mtry_values <- c(
  floor(sqrt(p)),
  floor(p / 3),
  floor(p / 2),
  p
)

rf_results <- data.frame(mtry = integer(), RMSE = numeric())

for (m in mtry_values) {
  
  rf_temp <- randomForest(
    log_salary ~ .,
    data = train_data,
    ntree = 500,
    mtry = m
  )
  
  preds <- predict(rf_temp, newdata = test_data)
  
  rmse <- sqrt(mean((test_data$log_salary - preds)^2))
  
  rf_results <- rbind(
    rf_results,
    data.frame(mtry = m, RMSE = rmse)
  )
}

rf_results

rf_results[which.min(rf_results$RMSE), ]

```
RMSE only changes by 1-2% when tuned so i am not going to dedicate too much time tuning it. when mtry is 8 the model performs best so that will be the threshold.

# Lasso Regression
```{r}
X_train <- model.matrix(log_salary ~., data = train_data)[, -1]
y_train <- train_data$log_salary

X_test <- model.matrix(log_salary ~., data = test_data)[, -1]
y_test <- test_data$log_salary
```

```{r}
lasso_model <- cv.glmnet(
  X_train,
  y_train,
  alpha =1,
  nfolds = 10
)

lasso_model
```

```{r}
lasso_model$lambda.min
lasso_model$lambda.1se
```

I'll use lamda.min because its best for predictive performance

```{r}
lasso_lamda.min <- glmnet(
  X_train,
  y_train,
  alpha = 1,
  lambda = lasso_model$lambda.min
)

lasso_lamda.min
```

```{r}
coef(lasso_lamda.min)
```
the benefit of lasso is that it does feature selection. so above we can see the coefficients that got excluded.

```{r}
preds_lasso <- predict(lasso_lamda.min, s = lasso_model$lambda.min, newx = X_test)

lasso_RMSE <- RMSE(preds_lasso, test_data$log_salary)
lasso_MAE <- MAE(preds_lasso, test_data$log_salary)
preds_lasso <- as.numeric(preds_lasso) # added this due to compatiblity issues
lasso_R2 <- R2(preds_lasso, test_data$log_salary)

evaluation <- rbind(
  evaluation,
  data.frame(
  model = "Lasso Regression",
  RMSE = lasso_RMSE,
  MAE = lasso_MAE,
  R_squared = lasso_R2,
  Adjusted_R_squared = NA #I'll come back to this if its relevant
  )
)

evaluation

```

# Model Selection

```{r}
evaluation
```

```{r}
library(ggplot2)

ggplot(evaluation, aes(x = model, y = RMSE, fill = model)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Model Comparison: RMSE", y = "RMSE") +
  scale_fill_brewer(palette = "Set2") +
  theme(legend.position = "none")

```
Comparing the three models, Random Forest achieved the best predictive performance with the lowest RMSE and MAE as well as the highest R², capturing non-linear relationships between player features and salary. Lasso regression improved slightly over the baseline linear model while performing feature selection, highlighting key predictors, but did not outperform Random Forest. Linear regression provides a simple, interpretable benchmark.

```{r}
saveRDS(rf_model, "../models/salary rf model.rds")
```

Initially I had a working dataset that i could use for modeling and a app deployment data set that had the working data and more. during EDA i realized salary data was missing for the majority of the players outside the MLS. During the project i learned that there are nuances of the MLS that plays a large role in determining a players salary. It would be false to assume thos nuances are the same in US leagues outside the MLS. All this is to say that I will be using the working data set in the app because the app deployment data i have contains players in the USL and NWSL. It would no be appropriate to assume the same pay structures of these leagues.

```{r}
# need to save levels and column names for shiny app
rf_xlevels <- rf_model$forest$xlevels
saveRDS(rf_xlevels, "../models/rf_xlevels.rds")

rf_vars <- colnames(train_data %>%
                      select(-log_salary)
                    )
saveRDS(rf_vars, "../models/rf_vars.rds")
```


