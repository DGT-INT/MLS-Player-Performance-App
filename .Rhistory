-`base salary`, # nearly replica of dependent variable
-`date of birth`, # already have age in the model
-nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
) %>%
mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
select(-`height (ft)`,
-`height (in)`,
-`games played`, # correlated to minutes played
-guaranteed_compensation,
-`goals + assists`, # already have both variables in the model
-`primary assists - exp. assists`, # already have both variables in the model
-`team name` # i want the model to learn from the individual players rather than the collected team
) %>%
mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
select(
-`gauranteed compensation`
)
set.seed(123456)
train_index <- createDataPartition(
working_data$log_salary,
p = 0.8,
list = FALSE
)
train_data <- working_data[train_index, ]
test_data <- working_data[-train_index, ]
nrow(train_data)
nrow(test_data)
lm_baseline <- lm(log_salary ~ .,
data = train_data)
summary(lm_baseline)
preds <- predict(lm_baseline, newdata = test_data)
lm_RMSE <- RMSE(preds, test_data$log_salary)
lm_RMSE
lm_MAE <- MAE(preds, test_data$log_salary)
lm_MAE
lm_R2 <- R2(preds, test_data$log_salary)
lm_R2
lm_ADJR2 <- summary(lm_baseline)$adj.r.squared
lm_ADJR2
evaluation <- data.frame(
model = "Linear Regression",
RMSE = lm_RMSE,
MAE = lm_MAE,
R_squared = lm_R2,
Adjusted_R_squared = lm_ADJR2
)
evaluation
# i was having syntatic issues so this resolves it.
names(train_data) <- make.names(names(train_data))
names(test_data)  <- make.names(names(test_data))
rf_model <- randomForest(
log_salary ~.,
data = train_data,
ntree = 500,
mtry = floor(sqrt(ncol(train_data)-1)),
importance = TRUE
)
rf_model
importance(rf_model)
varImpPlot(rf_model)
preds_rf <- predict(rf_model, newdata = test_data)
rf_RMSE <- RMSE(preds_rf, test_data$log_salary)
rf_MAE <- MAE(preds_rf, test_data$log_salary)
rf_R2 <- R2(preds_rf, test_data$log_salary)
evaluation <- rbind(
evaluation,
data.frame(
model = "Random Forest",
RMSE = rf_RMSE,
MAE = rf_MAE,
R_squared = rf_R2,
Adjusted_R_squared = NA #I'll come back to this if its relevant
)
)
evaluation
# testing different parameters to see if i should dedicate time on tuning the random forest
p <- ncol(train_data) - 1
mtry_values <- c(
floor(sqrt(p)),
floor(p / 3),
floor(p / 2),
p
)
rf_results <- data.frame(mtry = integer(), RMSE = numeric())
for (m in mtry_values) {
rf_temp <- randomForest(
log_salary ~ .,
data = train_data,
ntree = 500,
mtry = m
)
preds <- predict(rf_temp, newdata = test_data)
rmse <- sqrt(mean((test_data$log_salary - preds)^2))
rf_results <- rbind(
rf_results,
data.frame(mtry = m, RMSE = rmse)
)
}
rf_results
rf_results[which.min(rf_results$RMSE), ]
X_train <- model.matrix(log_salary ~., data = train_data)[, -1]
y_train <- train_data$log_salary
X_test <- model.matrix(log_salary ~., data = test_data)[, -1]
y_test <- test_data$log_salary
lasso_model <- cv.glmnet(
X_train,
y_train,
alpha =1,
nfolds = 10
)
lasso_model
lasso_model$lambda.min
lasso_model$lambda.1se
lasso_lamda.min <- glmnet(
X_train,
y_train,
alpha = 1,
lambda = lasso_model$lambda.min
)
lasso_lamda.min
coef(lasso_lamda.min)
preds_lasso <- predict(lasso_lamda.min, s = lasso_model$lambda.min, newx = X_test)
lasso_RMSE <- RMSE(preds_lasso, test_data$log_salary)
lasso_MAE <- MAE(preds_lasso, test_data$log_salary)
preds_lasso <- as.numeric(preds_lasso) # added this due to compatiblity issues
lasso_R2 <- R2(preds_lasso, test_data$log_salary)
evaluation <- rbind(
evaluation,
data.frame(
model = "Lasso Regression",
RMSE = lasso_RMSE,
MAE = lasso_MAE,
R_squared = lasso_R2,
Adjusted_R_squared = NA #I'll come back to this if its relevant
)
)
evaluation
evaluation
library(ggplot2)
ggplot(evaluation, aes(x = model, y = RMSE, fill = model)) +
geom_col() +
theme_minimal() +
labs(title = "Model Comparison: RMSE", y = "RMSE") +
scale_fill_brewer(palette = "Set2") +
theme(legend.position = "none")
model <- saveRDS(lasso_lamda.min, "../models/salary rf model.rds")
library(shiny); runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
View(working_data)
View(working_data)
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
`03 working data season 2025` <- readRDS("~/Documents/DGT-International/Projects/Player_Performance_App/data/03 working data season 2025.rds")
View(`03 working data season 2025`)
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
rf_xlevels <- rf_model$forest$xlevels
```
rf_xlevels <- rf_model$forest$xlevels
rf_xlevels
rf_vars <- colnames(train_data %>%
select(-log_salary)
)
rf_vars
saveRDS(rf_vars, "../models/rf_vars.rds")
length(rf_vars)
length(rf_xlevels)
# need to save levels and column names for shiny app
rf_xlevels <- rf_model$forest$xlevels
saveRDS(rf_xlevels, "../models/rf_xlevels.rds")
rf_vars <- colnames(train_data %>%
select(-log_salary)
)
saveRDS(rf_vars, "../models/rf_vars.rds")
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
rf_xlevels
lasso_lamda.min
class(lasso_lamda.min)
runApp('~/Documents/DGT-International/Projects/Player_Performance_App/Player_Performance_Lab.R')
data_2025 <- readRDS("data/03 working data season 2025.rds")
data_2025 <- readRDS("data/03 working data season 2025.rds")
setwd("..")
data_2025 <- readRDS("data/03 working data season 2025.rds")
rf_model <- readRDS("models/salary rf model.rds")
rf_vars <-  readRDS("models/rf_vars.rds")
rf_xlevels <-  readRDS("models/rf_xlevels.rds")
setdiff(rf_vars, names(data_2025))
View(data_2025)
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
View(evaluation)
model <- saveRDS(rf_model, "../models/salary rf model.rds")
saveRDS(rf_model, "../models/salary rf model.rds")
runApp('Player_Performance_Lab.R')
class(rf_model)
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
working_data <- readRDS("../data/03 working data season 2025.rds")
sapply(working_data, function(x) {
if (is.factor(x)) length(unique(x))
})
# limited season and competition to 1 so i am filtering them out
working_data <- working_data %>%
select(-season, # limited to only 2025 season
-competition, # limited to only MLS
#-`player name`, # irrelevant for modeling
-position, # already have general position
-`base salary`, # nearly replica of dependent variable
-`date of birth`, # already have age in the model
-nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
) %>%
mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
select(-`height (ft)`,
-`height (in)`,
-`games played`, # correlated to minutes played
-guaranteed_compensation,
-`goals + assists`, # already have both variables in the model
-`primary assists - exp. assists`, # already have both variables in the model
-`team name` # i want the model to learn from the individual players rather than the collected team
) %>%
mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
select(
-`gauranteed compensation`
)
set.seed(123456)
train_index <- createDataPartition(
working_data$log_salary,
p = 0.8,
list = FALSE
)
train_data <- working_data[train_index, ]
test_data <- working_data[-train_index, ]
nrow(train_data)
nrow(test_data)
lm_baseline <- lm(log_salary ~ .,
data = train_data)
summary(lm_baseline)
preds <- predict(lm_baseline, newdata = test_data)
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
working_data <- readRDS("../data/03 working data season 2025.rds")
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
working_data <- readRDS("../data/03 working data season 2025.rds")
sapply(working_data, function(x) {
if (is.factor(x)) length(unique(x))
})
# limited season and competition to 1 so i am filtering them out
working_data <- working_data %>%
select(-season, # limited to only 2025 season
-competition, # limited to only MLS
#-`player name`, # irrelevant for modeling
-position, # already have general position
-`base salary`, # nearly replica of dependent variable
-`date of birth`, # already have age in the model
-nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
) %>%
mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
select(-`height (ft)`,
-`height (in)`,
-`games played`, # correlated to minutes played
-guaranteed_compensation,
-`goals + assists`, # already have both variables in the model
-`primary assists - exp. assists`, # already have both variables in the model
-`team name` # i want the model to learn from the individual players rather than the collected team
) %>%
mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
select(
-`gauranteed compensation`
)
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
working_data <- readRDS("../data/03 working data season 2025.rds")
sapply(working_data, function(x) {
if (is.factor(x)) length(unique(x))
})
# limited season and competition to 1 so i am filtering them out
working_data <- working_data %>%
select(-season, # limited to only 2025 season
-competition, # limited to only MLS
#-`player name`, # irrelevant for modeling
-position, # already have general position
-`base salary`, # nearly replica of dependent variable
-`date of birth`, # already have age in the model
-nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
) %>%
mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
select(-`height (ft)`,
-`height (in)`,
-`games played`, # correlated to minutes played
-guaranteed_compensation,
-`goals + assists`, # already have both variables in the model
-`primary assists - exp. assists`, # already have both variables in the model
-`team name` # i want the model to learn from the individual players rather than the collected team
) %>%
mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
select(
-`gauranteed compensation`
)
set.seed(123456)
train_index <- createDataPartition(
working_data$log_salary,
p = 0.8,
list = FALSE
)
train_data <- working_data[train_index, ]
test_data <- working_data[-train_index, ]
nrow(train_data)
nrow(test_data)
lm_baseline <- lm(log_salary ~ .,
data = train_data)
summary(lm_baseline)
preds <- predict(lm_baseline, newdata = test_data)
library(tidyverse)
library(caret)
library(randomForest)
library(glmnet)
working_data <- readRDS("../data/03 working data season 2025.rds")
sapply(working_data, function(x) {
if (is.factor(x)) length(unique(x))
})
# limited season and competition to 1 so i am filtering them out
working_data <- working_data %>%
select(-season, # limited to only 2025 season
-competition, # limited to only MLS
-`player name`, # irrelevant for modeling
-position, # already have general position
-`base salary`, # nearly replica of dependent variable
-`date of birth`, # already have age in the model
-nationality, # too many individual factors. I reduced it to a binary home-grown flag to match MLS league rules
) %>%
mutate("height (cm)" = 30.48 * `height (ft)` + 2.54 * `height (in)`) %>% # consolidating height in 1 variable
select(-`height (ft)`,
-`height (in)`,
-`games played`, # correlated to minutes played
-guaranteed_compensation,
-`goals + assists`, # already have both variables in the model
-`primary assists - exp. assists`, # already have both variables in the model
-`team name` # i want the model to learn from the individual players rather than the collected team
) %>%
mutate(log_salary = log(`gauranteed compensation`)) %>% # modeling on a log scale to address skewness and heteroskedascity
select(
-`gauranteed compensation`
)
set.seed(123456)
train_index <- createDataPartition(
working_data$log_salary,
p = 0.8,
list = FALSE
)
train_data <- working_data[train_index, ]
test_data <- working_data[-train_index, ]
nrow(train_data)
nrow(test_data)
lm_baseline <- lm(log_salary ~ .,
data = train_data)
summary(lm_baseline)
preds <- predict(lm_baseline, newdata = test_data)
lm_RMSE <- RMSE(preds, test_data$log_salary)
lm_RMSE
lm_MAE <- MAE(preds, test_data$log_salary)
lm_MAE
lm_R2 <- R2(preds, test_data$log_salary)
lm_R2
lm_ADJR2 <- summary(lm_baseline)$adj.r.squared
lm_ADJR2
evaluation <- data.frame(
model = "Linear Regression",
RMSE = lm_RMSE,
MAE = lm_MAE,
R_squared = lm_R2,
Adjusted_R_squared = lm_ADJR2
)
evaluation
# i was having syntatic issues so this resolves it.
names(train_data) <- make.names(names(train_data))
names(test_data)  <- make.names(names(test_data))
rf_model <- randomForest(
log_salary ~.,
data = train_data,
ntree = 500,
mtry = floor(sqrt(ncol(train_data)-1)),
importance = TRUE
)
rf_model
importance(rf_model)
varImpPlot(rf_model)
preds_rf <- predict(rf_model, newdata = test_data)
rf_RMSE <- RMSE(preds_rf, test_data$log_salary)
rf_MAE <- MAE(preds_rf, test_data$log_salary)
rf_R2 <- R2(preds_rf, test_data$log_salary)
evaluation <- rbind(
evaluation,
data.frame(
model = "Random Forest",
RMSE = rf_RMSE,
MAE = rf_MAE,
R_squared = rf_R2,
Adjusted_R_squared = NA #I'll come back to this if its relevant
)
)
evaluation
# testing different parameters to see if i should dedicate time on tuning the random forest
p <- ncol(train_data) - 1
mtry_values <- c(
floor(sqrt(p)),
floor(p / 3),
floor(p / 2),
p
)
rf_results <- data.frame(mtry = integer(), RMSE = numeric())
for (m in mtry_values) {
rf_temp <- randomForest(
log_salary ~ .,
data = train_data,
ntree = 500,
mtry = m
)
preds <- predict(rf_temp, newdata = test_data)
rmse <- sqrt(mean((test_data$log_salary - preds)^2))
rf_results <- rbind(
rf_results,
data.frame(mtry = m, RMSE = rmse)
)
}
rf_results
rf_results[which.min(rf_results$RMSE), ]
X_train <- model.matrix(log_salary ~., data = train_data)[, -1]
y_train <- train_data$log_salary
X_test <- model.matrix(log_salary ~., data = test_data)[, -1]
y_test <- test_data$log_salary
lasso_model <- cv.glmnet(
X_train,
y_train,
alpha =1,
nfolds = 10
)
lasso_model
lasso_model$lambda.min
lasso_model$lambda.1se
lasso_lamda.min <- glmnet(
X_train,
y_train,
alpha = 1,
lambda = lasso_model$lambda.min
)
lasso_lamda.min
coef(lasso_lamda.min)
preds_lasso <- predict(lasso_lamda.min, s = lasso_model$lambda.min, newx = X_test)
lasso_RMSE <- RMSE(preds_lasso, test_data$log_salary)
lasso_MAE <- MAE(preds_lasso, test_data$log_salary)
preds_lasso <- as.numeric(preds_lasso) # added this due to compatiblity issues
lasso_R2 <- R2(preds_lasso, test_data$log_salary)
evaluation <- rbind(
evaluation,
data.frame(
model = "Lasso Regression",
RMSE = lasso_RMSE,
MAE = lasso_MAE,
R_squared = lasso_R2,
Adjusted_R_squared = NA #I'll come back to this if its relevant
)
)
evaluation
evaluation
library(ggplot2)
ggplot(evaluation, aes(x = model, y = RMSE, fill = model)) +
geom_col() +
theme_minimal() +
labs(title = "Model Comparison: RMSE", y = "RMSE") +
scale_fill_brewer(palette = "Set2") +
theme(legend.position = "none")
saveRDS(rf_model, "../models/salary rf model.rds")
# need to save levels and column names for shiny app
rf_xlevels <- rf_model$forest$xlevels
saveRDS(rf_xlevels, "../models/rf_xlevels.rds")
rf_vars <- colnames(train_data %>%
select(-log_salary)
)
saveRDS(rf_vars, "../models/rf_vars.rds")
class(rf_model)
library(shiny); runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
runApp('Player_Performance_Lab.R')
